{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_pytorch_classification_cifar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "811e56af8abd4eaa853c64f00ec38153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_22f89672eff84a77be2f3f780ca9d843",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0d95458c26d749d9af684631ba56d5d9",
              "IPY_MODEL_ceff28e26c1b4c58982b5e2fcf41a647"
            ]
          }
        },
        "22f89672eff84a77be2f3f780ca9d843": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0d95458c26d749d9af684631ba56d5d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bb3ac851b14d4c98b5b5295bd63df8c4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_933230aa48b8455d81cdc311409dc2d6"
          }
        },
        "ceff28e26c1b4c58982b5e2fcf41a647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b3a7728b8462493298112306e093b697",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:07&lt;00:00, 22605789.16it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0e42e550f3fd4de39a6c1962f9685f41"
          }
        },
        "bb3ac851b14d4c98b5b5295bd63df8c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "933230aa48b8455d81cdc311409dc2d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b3a7728b8462493298112306e093b697": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0e42e550f3fd4de39a6c1962f9685f41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BviI4QIdWiq"
      },
      "source": [
        "<p align=\"center\"><img width=\"50%\" src=\"https://aimodelsharecontent.s3.amazonaws.com/aimodshare_banner.jpg\" /></p>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuAkm2rldeUv"
      },
      "source": [
        "<p align=\"center\"><h1 align=\"center\">Flower Image Classification with PyTorch</h1> <h3 align=\"center\">(Prepare to deploy model and preprocessor to REST API/Web Dashboard in four easy steps...)</h3></p>\n",
        "<p align=\"center\"><img width=\"80%\" src='https://drive.google.com/thumbnail?id=1ea5R66cqAXs3g4oIeEiUUMhictrL2IBg&sz=w1200-h1200' /></p>\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBoJu8oUk8UU"
      },
      "source": [
        "## **(1) Train Model Using `torch`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWt5CNFznF64"
      },
      "source": [
        "We use the standard CIFAR-10 dataset, and train it just the way it is done in the eponymous CIFAR-10 `torch` tutorial. The caveat is that we will be loading images in `batch_size=1` inside the `DataLoader()`. NB CIFAR-10 has images of size 3x32x32 (3-channel/color images of 32x32 pixels in size)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsl62dqMC92n"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqQqCYu9p2yS"
      },
      "source": [
        "`torch` datasets are PILImage images of range [0, 1]. `ToTensor()` transforms to tensors of normalized range [-1, 1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg7Rcs0rTfmi",
        "outputId": "7d9c768f-e6e8-4dfb-93b6-2766c030f5b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "811e56af8abd4eaa853c64f00ec38153",
            "22f89672eff84a77be2f3f780ca9d843",
            "0d95458c26d749d9af684631ba56d5d9",
            "ceff28e26c1b4c58982b5e2fcf41a647",
            "bb3ac851b14d4c98b5b5295bd63df8c4",
            "933230aa48b8455d81cdc311409dc2d6",
            "b3a7728b8462493298112306e093b697",
            "0e42e550f3fd4de39a6c1962f9685f41"
          ]
        }
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1, # 50000 items.\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=1, # 10000 items.\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "811e56af8abd4eaa853c64f00ec38153",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75rEs_sgTrx6",
        "outputId": "0411fa45-4a9e-420b-b077-7870f8838e0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Functions to show an image.\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5 # Unnormalize.\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Get some random training images.\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# Show images.\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# Print labels.\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(1)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " deer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXRkKwgwqQtC"
      },
      "source": [
        "Let's do a plain vanilla neural net that takes our color (3-channel) images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2g3ycuQT0BT"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS9yFzsRT3s-"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() # Define a Loss function.\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) # Define an optimizer."
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viVOPzl_tE2I"
      },
      "source": [
        "Loop over the data iterator, feed the inputs to the network and optimize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfcf4TciTfpN",
        "outputId": "154b03bd-4c60-4784-9ec4-7f53f7d9fd51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "for epoch in range(2):  # Loop over the dataset multiple times.\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # Get the inputs; data is a list of [inputs, labels].\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Zero the parameter gradients.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize.\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999: # Print every 2000 mini-batches.\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 2.258\n",
            "[1,  4000] loss: 2.067\n",
            "[1,  6000] loss: 1.930\n",
            "[1,  8000] loss: 1.886\n",
            "[1, 10000] loss: 1.806\n",
            "[1, 12000] loss: 1.754\n",
            "[1, 14000] loss: 1.689\n",
            "[1, 16000] loss: 1.718\n",
            "[1, 18000] loss: 1.677\n",
            "[1, 20000] loss: 1.679\n",
            "[1, 22000] loss: 1.668\n",
            "[1, 24000] loss: 1.645\n",
            "[1, 26000] loss: 1.648\n",
            "[1, 28000] loss: 1.619\n",
            "[1, 30000] loss: 1.603\n",
            "[1, 32000] loss: 1.640\n",
            "[1, 34000] loss: 1.587\n",
            "[1, 36000] loss: 1.588\n",
            "[1, 38000] loss: 1.553\n",
            "[1, 40000] loss: 1.556\n",
            "[1, 42000] loss: 1.603\n",
            "[1, 44000] loss: 1.592\n",
            "[1, 46000] loss: 1.562\n",
            "[1, 48000] loss: 1.553\n",
            "[1, 50000] loss: 1.544\n",
            "[2,  2000] loss: 1.521\n",
            "[2,  4000] loss: 1.544\n",
            "[2,  6000] loss: 1.534\n",
            "[2,  8000] loss: 1.553\n",
            "[2, 10000] loss: 1.543\n",
            "[2, 12000] loss: 1.506\n",
            "[2, 14000] loss: 1.528\n",
            "[2, 16000] loss: 1.488\n",
            "[2, 18000] loss: 1.555\n",
            "[2, 20000] loss: 1.484\n",
            "[2, 22000] loss: 1.510\n",
            "[2, 24000] loss: 1.572\n",
            "[2, 26000] loss: 1.524\n",
            "[2, 28000] loss: 1.514\n",
            "[2, 30000] loss: 1.504\n",
            "[2, 32000] loss: 1.502\n",
            "[2, 34000] loss: 1.523\n",
            "[2, 36000] loss: 1.476\n",
            "[2, 38000] loss: 1.506\n",
            "[2, 40000] loss: 1.487\n",
            "[2, 42000] loss: 1.462\n",
            "[2, 44000] loss: 1.515\n",
            "[2, 46000] loss: 1.448\n",
            "[2, 48000] loss: 1.481\n",
            "[2, 50000] loss: 1.570\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q06ktW4HlRCu"
      },
      "source": [
        "## **(2) Save Model to ONNX**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSuCVjabPqt_"
      },
      "source": [
        "model = Net()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu2XmrCGR6cV",
        "outputId": "a023a8fd-c180-41f3-8ca2-db7b65bbdd85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hb88jVmUO1UH"
      },
      "source": [
        "dummy_input = next(iter(testloader))[0] # First observation from the batch. Tensor with shape torch.Size([1, 3, 32, 32])."
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78azKt1_OgPR"
      },
      "source": [
        "torch.onnx.export(\n",
        "    model.cpu(),\n",
        "    dummy_input,\n",
        "    'my_model.onnx',\n",
        "    export_params=True,\n",
        "    do_constant_folding=True,\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBOt1rFZllpf"
      },
      "source": [
        "## **(3) Write a Preprocessor Function**\n",
        "\n",
        "> ### Preprocessor functions for image prediction models can use ***`cv2`*** and ***`numpy`*** to read in and preprocess images. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj52M0FXwApr"
      },
      "source": [
        "Since we pre-process the images using simple packages, we need to read in the CIFAR-10 dataset from the previously downloaded `cifar-10-batches-py`. Since we rely on `os` and `pickle`, we do this ouside the pre-processor function. The output is an `ndarray` for the target (images) and a list for the labels. We only load in the test-set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKkWetDemtF6"
      },
      "source": [
        "img_rows, img_cols = 32, 32\n",
        "input_shape = (img_rows, img_cols, 3)\n",
        "\n",
        "def load_CIFAR_batch(filename):\n",
        "    \"\"\"\n",
        "    Load a single batch of CIFAR-10.\n",
        "    \"\"\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        datadict = pickle.load(f, encoding='latin1') # Python 3 and newer.\n",
        "        X = datadict['data']\n",
        "        y = datadict['labels']\n",
        "        return X, y\n",
        "\n",
        "def load_CIFAR10(ROOT):\n",
        "    \"\"\"\n",
        "    Load all, in this case, the test batch of CIFAR-10.\n",
        "    \"\"\"\n",
        "    f = os.path.join(ROOT, 'test_batch') # Load the test_batch.\n",
        "    X, y = load_CIFAR_batch(f)\n",
        "    return X, y"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m48N-l8XmtD2"
      },
      "source": [
        "cifar10_dir = '/content/data/cifar-10-batches-py' # Replace with your path.\n",
        "\n",
        "import os\n",
        "from six.moves import cPickle as pickle\n",
        "\n",
        "X, y = load_CIFAR10(cifar10_dir) # X images (targets), y labels."
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2o7NddAtWI0"
      },
      "source": [
        "`torch` tensors assume color channel is the **first** dimension, whereas `matplotlib` assumes it's the **third** dimension. Therefore always convert images (H x W x **C**) to a `torch.Tensor` of shape (**C** x H x W), and vice-versa for plotting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn4kSvNF252r"
      },
      "source": [
        "def preprocessor(target, label):\n",
        "        \"\"\"\n",
        "        This function preprocesses images in the form of ndarray, reshapes, normalizes and\n",
        "        converts them to float32 for ONNX before unstacking to get the requisite format for runtime.\n",
        "        \n",
        "        params:\n",
        "            target\n",
        "                loaded images as ndarray\n",
        "            label\n",
        "                list of labels from the loader\n",
        "                      \n",
        "        returns:\n",
        "            preprocessed_X, preprocessed_y\n",
        "                preprocessed image data\n",
        "                  \n",
        "        \"\"\"\n",
        "           \n",
        "        import numpy as np\n",
        "\n",
        "        X = target; y = label # Read in the targets, labels.\n",
        "        X = X.reshape(10000, 3, 32, 32) # Reshape to move channels last-to-first.\n",
        "        y = np.array(y)\n",
        "        X = X.astype('float32'); y = y.astype('float32') # float32 for ONNX.\n",
        "        X /= 255 # Normalization.\n",
        "        *X, = X; *y, = y; preprocessed_y = [int(i) for i in y] # Use the star operator to un-stack a numpy array.\n",
        "        preprocessed_X = [np.expand_dims(i, axis=0) for i in X] # Adding 1 to object shape to finalize expected run-time prediction shape.                                                                            \n",
        "\n",
        "        return preprocessed_X, preprocessed_y"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJQTH9UvmtBj"
      },
      "source": [
        "testdata, testlabels = preprocessor(target=X, label=y)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAwbLXqDtwD3"
      },
      "source": [
        "### **Test Model Output Using `onnxruntime`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9ThQhDDRGuL",
        "outputId": "ceb04bf5-ab3a-475b-c923-7a6624fc1577",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "!pip install onnxruntime"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting onnxruntime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/a5/a70b05bc5a6037e0bf29d21828945a49fa4d341690c8ae7f01a62a177a2b/onnxruntime-1.5.2-cp36-cp36m-manylinux2014_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from onnxruntime) (1.18.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnxruntime) (3.12.4)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnxruntime) (50.3.0)\n",
            "Installing collected packages: onnxruntime\n",
            "Successfully installed onnxruntime-1.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8EWIfvURPfS",
        "outputId": "e675329a-1b1a-44e8-fe6d-2a70b397d51d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import onnxruntime as rt\n",
        "modeltest = rt.InferenceSession('my_model.onnx')\n",
        "\n",
        "input_name = modeltest.get_inputs()[0].name\n",
        "input_data=testdata[4] # Needs to be float32. Try e.g. the fourth image from testdata.\n",
        "\n",
        "res=modeltest.run(None,  {input_name: input_data})\n",
        "res[0]\n",
        "prob = res[0]\n",
        "\n",
        "def predict_classes(x): # Adjusted from Keras GitHub code.\n",
        "        proba=x\n",
        "        if proba.shape[-1] > 1:\n",
        "          return proba.argmax(axis=-1)\n",
        "        else:\n",
        "          return (proba > 0.5).astype('int32')\n",
        "\n",
        "prediction_index=predict_classes(prob)\n",
        "def index_to_label(labels,index_n):\n",
        "    return labels[index_n]\n",
        "labels=['plane', 'car', 'bird', 'cat',\n",
        "        'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "result=list(map(lambda x: labels[x], prediction_index))\n",
        "\n",
        "result"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['frog']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1ZgQTsrRbcx"
      },
      "source": [
        "### **Display a Test Model Output**\n",
        "\n",
        "> #### Optional. NB The image may or may not match depending on how well the model learned on the training set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjkDt0sVXTVN"
      },
      "source": [
        "testpic = testdata[4] # Try e.g. the fourth image from testdata.\n",
        "testpic = torch.from_numpy(testpic)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwTQ8XiTvlpx"
      },
      "source": [
        "Remember, shape (H x W x **C**) for plotting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZChhhJxems3j",
        "outputId": "e321bd73-a15e-41ea-c3b2-42e0563c0ccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5 # Unnormalize.\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Show images.\n",
        "imshow(torchvision.utils.make_grid(testpic))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhvNRTx4lxbc"
      },
      "source": [
        "## **(4) Save Preprocessor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xsfDZajhgbm"
      },
      "source": [
        "# ! pip3 install aimodelshare"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij0Dh8afhgYZ"
      },
      "source": [
        "def export_preprocessor(preprocessor_function, filepath):\n",
        "    import dill\n",
        "    with open(filepath, \"wb\") as f:\n",
        "        dill.dump(preprocessor_function, f)\n",
        "\n",
        "# import aimodelshare as ai # Once we can deploy this, we use it in lieu of the below.\n",
        "# ai.export_preprocessor(preprocessor, \"preprocessor.pkl\")\n",
        "\n",
        "export_preprocessor(preprocessor, \"preprocessor.pkl\")"
      ],
      "execution_count": 48,
      "outputs": []
    }
  ]
}